{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. 손실함수 구현하기 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## sum_of_squares error의 경우에는 (y-t)^2 * 0.5인 것을 이용\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t) ** 2)\n",
    "\n",
    "## cross_entropy_error의 경우에는 원-핫 인코딩의 형태로 바꿔주는 거 필요\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        t = t.reshape(1, t.size)\n",
    "        \n",
    "    if y.size == t.size:\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. 수치미분 구현하기 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 ## 0이 되는 거 방지\n",
    "    grads = np.zeros_like(x) ## x의 shape와 동일한 것 생성 (이유 : gradient 저장)\n",
    "    \n",
    "    ## 편미분을 iterative하게 진행하기 위하여 nditer를 이용하여 미분 진행하기\n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    while not it.finished():\n",
    "        idx = it.multi_index\n",
    "        ## idx는 현재 인덱스임. ex) 2 x 3행렬에선 (1,1), (1, 2), (1, 3) 이런 식\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grads[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        return grads\n",
    "    \n",
    "## gradient descent 식에 맞춰서 구현하면 됨. \n",
    "def gradient_descent(f, init_x, lr, step_num):\n",
    "    x = init_x\n",
    "    \n",
    "    for idx in range(step_num):\n",
    "        grad = numerical_gradient(f, x) ## 각 idx 마다 미분값 도출\n",
    "        x = x - lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05320000000000002, 0.3566748010815999)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.7, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.06])\n",
    "\n",
    "sum_squares_error(y, t), cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. 2층 신경망을 통해 가중치 업데이트 하기 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        ## 입력받는 숫자에 맞게 2층 신경망 구현하기\n",
    "        self.params = {} ## 파라미터를 저장하는 딕셔너리 변수\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = weight_init_std * np.random.randn(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = weight_init_std * np.random.randn(output_size)\n",
    "        \n",
    "    ## init에 저장하였던 파라미터들을 이용하여 순전파 계산하기    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2'] ## 각각 weight\n",
    "        b1, b2 = self.params['b1'], self.params['b2'] ## 각각 편향\n",
    "        \n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1 ## 입력층 to 은닉층\n",
    "        z1 = sigmoid(a1) ## 편향\n",
    "        a2 = np.dot(z1, W2) + b2 ## 은닉층 to 출력층\n",
    "        y = softmax(a2) ## softmax 이용하여 최종 라벨 도출\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    ## 예측한 것 정확도 확인\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x) ## 순전파 계산\n",
    "        y = np.argmax(y, axis = 1) ## 라벨값(예측값) 도출\n",
    "        t = np.argmax(t, axis = 1) ## 실제값\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) ## 정확도 계산\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    ## 손실 계산\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x) ## 순전파 계산을 통해 예측값 도출\n",
    "        \n",
    "        return cross_entropy_error(y, t) ## 예측값과 실제값 사이에 gradient 도출\n",
    "    \n",
    "    ## gradient descent에 이용할 gradient 계산\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t) ## loss 계산하기\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.19, 0.1184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('C://Users/revol/Deep Learning/HW/dataset')\n",
    "from mnist import load_mnist\n",
    "from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "batch_size = 100\n",
    "train_size = x_train.shape[0]\n",
    "iter_num = 2\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "init_W1 = network.params['W1']\n",
    "\n",
    "for iter in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if iter % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_batch, t_batch)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc, test acc | ' + str(train_acc) + ', '+ str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. 3층 신경망 구현하기 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50) (50,) (50, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class ThreeLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {} ## 2층 신경망과 동일한 방법으로 파라미터 초기화\n",
    "        \n",
    "        ## 은닉층이 2개가 포함되기 때문에, 배열을 이용하여 첫 번째와 두 번째 은닉층 구별\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size[0])\n",
    "        self.params['b1'] = weight_init_std * np.random.randn(hidden_size[0])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size[0], hidden_size[1])\n",
    "        self.params['b2'] = weight_init_std * np.random.randn(hidden_size[1])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size[1], output_size)\n",
    "        self.params['b3'] = weight_init_std * np.random.randn(output_size)\n",
    "        \n",
    "    def predict(self, x): ## 순전파 계산 진행 (2층 신경망과 동일한 방법)\n",
    "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2, W3) + b3\n",
    "        y = softmax(a3)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "net = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "print(net.params['W2'].shape, net.params['b2'].shape, net.params['W3'].shape, net.params['b3'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업데이트된 W1의 가중치 :  [[-0.02293887 -0.02129477 -0.00916254 ...  0.00477386 -0.00622735\n",
      "  -0.0118936 ]\n",
      " [-0.0105399  -0.02015497 -0.00522983 ... -0.02797655 -0.00894673\n",
      "   0.00579542]\n",
      " [ 0.01038812 -0.01574766 -0.00011639 ... -0.01864678 -0.02131355\n",
      "   0.00587509]\n",
      " ...\n",
      " [-0.00774502  0.00380826 -0.00392613 ...  0.00920175 -0.01071928\n",
      "  -0.00867553]\n",
      " [ 0.00557131  0.02119032  0.0091312  ...  0.00886956 -0.01722848\n",
      "   0.02901   ]\n",
      " [-0.00112854 -0.00897207  0.02043213 ... -0.01263106  0.00472207\n",
      "  -0.01842378]]\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = x_train.shape[0]\n",
    "iter_num = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "init_W1 = network.params['W1']\n",
    "for iter in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        net.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "\n",
    "print('업데이트된 W1의 가중치 : ', net.params['W1'] - init_W1)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
