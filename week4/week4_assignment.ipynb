{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. ì†ì‹¤í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì†ì‹¤í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ë¨¼ì €, ì˜¤ì°¨ì œê³±í•©ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤. ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ì°¸ê³ í•˜ì—¬ ì˜¤ì°¨ ì œê³±í•©ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì„ êµ¬í˜„í•˜ì„¸ìš”. í•¨ìˆ˜ ì›í˜•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.(y, tëŠ” ê°ê° ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’)\n",
    "def sum_squares_error(y, t): #ì •ë‹µë ˆì´ë¸”ë¿ë§Œ ì•„ë‹ˆë¼, ì˜¤ë‹µë ˆì´ë¸”ì˜ ìˆ˜ì¹˜ë„ ë°˜ì˜(ì›í•«ì¸ì½”ë”©)\n",
    "    return (0.5) * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) ì¶”ê°€ë¡œ, êµì°¨ ì—”íŠ¸ë¡œí”¼(cross entropy)ë¥¼ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì„ ì°¸ê³ í•˜ì—¬ êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„ êµ¬í˜„í•˜ì„¸ìš”. í•¨ìˆ˜ ì›í˜•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (ë‹¨, tkì™€ ykëŠ” ê°ê° kë²ˆì§¸ ë°ì´í„°ì˜ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’)\n",
    "#ì •ë‹µë ˆì´ë¸”ì´ ì›í•«ì¸ì½”ë”©ì¸ ê²½ìš°\n",
    "def cross_entropy_error_one_hot(y, t):\n",
    "    if y.dim==1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta)) / batch_size #ì •ë‹µë¼ë²¨ë§ ì´ì™¸ì˜ ê°’ì€ ë²„ë ¤ì§(*0)\n",
    "\n",
    "\n",
    "#ì •ë‹µë ˆì´ë¸”ì´ ë‹¨ì¼ê°’ì¸ ê²½ìš°\n",
    "def cross_entropy_error(y, t): #ì´ êµ¬í˜„ì˜ ê²½ìš° ì›í•«ì¸ì½”ë”©ì—ì„œ tê°€ 0ì¸ ì›ì†ŒëŠ” êµì°¨ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë„ 0ì´ë¯€ë¡œ ê·¸ ê³„ì‚°ì„ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ í•µì‹¬. ì¦‰ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥ë§Œìœ¼ë¡œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ê³„ì‚°!\n",
    "    if y.dim==1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size), t] + delta)) / batch_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. ìˆ˜ì¹˜ ë¯¸ë¶„ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ì´ì œ í¸ë¯¸ë¶„ì„ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ëª©ì í•¨ìˆ˜ fì™€ ë¯¸ì§€ìˆ˜ xê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í¸ë¯¸ë¶„ì„ í•˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•˜ì„¸ìš”. í•¨ìˆ˜ ì›í˜•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (ë‹¨, êµ¬í˜„í•˜ì‹¤ ë•Œ ê°€ì¤‘ì¹˜ê°€ 1ì°¨ì›ì˜ í˜•íƒœê°€ ì•„ë‹Œ 2ì°¨ì›ì˜ í˜•íƒœê°€ ë“¤ì–´ì™”ë‹¤ê³  ê°€ì •í•˜ê³  êµ¬í˜„í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.) hint : np.nditer í™œìš©\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # ê°’ ë³µì›\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ê¸° ìœ„í•´ ê²½ì‚¬ í•˜ê°•ë²•(gradient descent)ë¥¼ ì´ìš©í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê²½ì‚¬í•˜ê°•ë²•ì„ ì ìš©í•  ë•Œ ê°±ì‹ ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ì˜ ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ğ’™ğ’Œ+ğŸ = ğ’™ğ’Œ âˆ’ ğœ¶ Ã— ğğ’‡/ğğ’™ğ’Œ ìœ„ ì‹ì„ ì´ìš©í•˜ì—¬ ê²½ì‚¬ í•˜ê°•ë²•ì„ êµ¬í˜„í•˜ì„¸ìš”. í•¨ìˆ˜ ì›í˜•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (f : ëª©ì í•¨ìˆ˜, init_x : ë§¤ê°œë³€ìˆ˜ì˜ ì´ˆê¸°ê°’, lr : alpha(í•™ìŠµë¥ ), step_num : ê²½ì‚¬í•˜ê°•ë²•ì„ ì§„í–‰í•  íšŸìˆ˜)\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x) #\"ì´ë§Œí¼ ì´ë™í•˜ì„¸ìš”\"\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 2ì¸µ ì‹ ê²½ë§ì„ í†µí•´ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸í•˜ê¸°\n",
    "MNIST ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ì‹ ê²½ë§ì„ êµ¬í˜„í•´ ë³´ë ¤ê³  í•©ë‹ˆë‹¤.(ìˆœì„œëŒ€ë¡œ ì•„ë˜ ìª½ ìˆ«ìëŠ” 784, 100, 10ì…ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ì´ì œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ 2ì¸µ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ì—¬ MNIST ë°ì´í„°ì— ëŒ€í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ TwoLayerNetì´ë¼ëŠ” í´ë˜ìŠ¤ ì•ˆì—ì„œ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤. Ipynb íŒŒì¼ ì•ˆì— TwoLayerNetì´ë¼ëŠ” ì´ë¦„ì˜ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì£¼ì„¸ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 2ë²ˆì—ì„œ ë§Œë“  TwoLayerNet í´ë˜ìŠ¤ ì•ˆì— ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê° í•¨ìˆ˜ì˜ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ì•Œë§ê²Œ ì½”ë“œë¥¼ êµ¬í˜„í•´ ì£¼ì„¸ìš”.(weight_init_stdë„ í™œìš©í•´ì£¼ì„¸ìš”)\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # a. í•´ë‹¹ í•¨ìˆ˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. Input_sizeëŠ” ì…ë ¥ì¸µì˜ ë…¸ë“œ, hidden_sizeëŠ” ì€ë‹‰ì¸µì˜ ë…¸ë“œ, output_sizeëŠ” ì¶œë ¥ì¸µì˜ ë…¸ë“œ ìˆ˜ ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•œ í›„ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”. (í•´ë‹¹ í•¨ìˆ˜ëŠ” í´ë˜ìŠ¤ì—ì„œ ì´ˆê¸°í™”ê°€ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— ë¦¬í„´ê°’ì´ ì—†ìŠµë‹ˆë‹¤.)\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    # b. í•´ë‹¹ í•¨ìˆ˜ëŠ” ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì´ìš©í•˜ì—¬ ì‹ ê²½ë§ì˜ ì—°ì‚° ê³¼ì •ì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ì§€ë‚œ ì£¼ì™€ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë¦¬í„´í•˜ë„ë¡ êµ¬í˜„í•˜ì„¸ìš”. ë¦¬í„´í•´ì•¼ í•˜ëŠ” ê²ƒì€ y(ì˜ˆì¸¡ê°’) ì…ë‹ˆë‹¤.\n",
    "    def predict(self, x):\n",
    "        W1,W2 = self.params['W1'], self.params['W2']\n",
    "        b1,b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # c. í•´ë‹¹ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. Xì™€ tëŠ” ê°ê° ì…ë ¥ ë°ì´í„°ì™€ ì •ë‹µ ë ˆì´ë¸”ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•ë„ accuracyë¥¼ ê³„ì‚°í•˜ê³  ë¦¬í„´í•˜ì„¸ìš”.\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # d. í•´ë‹¹ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ê³„ì‚° ê³¼ì •ì„ êµ¬í˜„í•˜ê³  lossë¥¼ ë¦¬í„´í•˜ì„¸ìš”. Loss functionì€ ììœ¨ì ìœ¼ë¡œ ì„ íƒí•˜ì‹œê³ , í•´ë‹¹ loss functionì„ ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    # e. í•´ë‹¹ í•¨ìˆ˜ëŠ” ê³„ì‚°í•œ lossë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¦¬í„´í•˜ë„ë¡ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# 4) ì´ì œ 3ë²ˆì—ì„œ êµ¬í˜„í•œ í´ë˜ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—°ì‚° ê³¼ì •ì„ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ì„œ ê°€ì¤‘ì¹˜ì˜ shapeê°€ ì˜¬ë°”ë¥´ê²Œ ì¶œë ¥ë˜ëŠ” ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "#from common.two_layer_net import * # ìœ„ í•¨ìˆ˜ì™€ ë˜‘ê°™ì§€ë§Œ ì¶©ëŒë°œìƒí•˜ê¸°ë•Œë¬¸ì— ì£¼ì„ì²˜ë¦¬\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.0944, 0.0974\n",
      "train acc, test acc : 0.4202, 0.4011\n",
      "train acc, test acc : 0.6146, 0.6053\n"
     ]
    }
   ],
   "source": [
    "# 6) ë³¸ê²©ì ìœ¼ë¡œ MNIST ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.(train data ì´ìš©) EpochsëŠ” 2ë²ˆë§Œ ëŒê³ , í•œ epochë‹¹ ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ì˜ í¬ê¸°(batch size)ëŠ” 100ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë˜í•œ, ê°€ì¤‘ì¹˜ w1ì´ ì´ˆê¸°ê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ” ì§€ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('')\n",
    "from dataset.mnist import load_mnist\n",
    "#from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "import copy\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "#ì¼ë¶€ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "subset_size = 10000\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "epoch_num = 2  # ì›í•˜ëŠ” epoch ìˆ˜\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #ë¹„êµë¥¼ ìœ„í•œ ì´ˆê¸°ê°’ ì €ì¥\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.8557096720821074\n",
      "-61.76200638274949\n",
      "57.90629671066738\n"
     ]
    }
   ],
   "source": [
    "# W1 ì´ˆê¸°ê°’ (ë¹„êµë¥¼ ìœ„í•´ sumì—°ì‚°ëœ ê°’ìœ¼ë¡œ ì¶œë ¥)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 í˜„ì¬ê°’\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# ì°¨ì´ ë¹„êµ\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.789917477035026,\n",
       " 9.402878615095732,\n",
       " 8.53129252003895,\n",
       " 8.956632884256209,\n",
       " 9.155539960960349,\n",
       " 9.17339682078703,\n",
       " 9.393068895443568,\n",
       " 9.32320104269466,\n",
       " 8.871058476502549,\n",
       " 8.748610338629673,\n",
       " 9.860914834073586,\n",
       " 8.708943996540238,\n",
       " 8.528062875459552,\n",
       " 9.190757737328298,\n",
       " 8.668473667912115,\n",
       " 8.74836678599071,\n",
       " 9.628166929402846,\n",
       " 9.014904746447389,\n",
       " 8.920513072717156,\n",
       " 8.636347240924136,\n",
       " 8.494875067946344,\n",
       " 8.504336803104428,\n",
       " 9.415620445784853,\n",
       " 8.532462989336825,\n",
       " 8.251651851354955,\n",
       " 8.334178857515965,\n",
       " 7.977338374885249,\n",
       " 8.508595628327468,\n",
       " 9.037225948140746,\n",
       " 8.52239796478214,\n",
       " 9.222595521753204,\n",
       " 9.103690415217098,\n",
       " 8.989468279386891,\n",
       " 7.676927544145608,\n",
       " 8.539393538062177,\n",
       " 8.981825598728424,\n",
       " 7.954785772585041,\n",
       " 7.660103009255141,\n",
       " 8.763949356144254,\n",
       " 8.86246195821568,\n",
       " 7.975586171879179,\n",
       " 7.947219773391989,\n",
       " 7.339098352617026,\n",
       " 9.056825281589106,\n",
       " 7.326638564813185,\n",
       " 7.176040996159823,\n",
       " 8.178553661202754,\n",
       " 7.7584252656771175,\n",
       " 7.359936547510157,\n",
       " 7.742716069808982,\n",
       " 7.458780545428644,\n",
       " 7.512987486508391,\n",
       " 7.268141924098149,\n",
       " 7.11724159923928,\n",
       " 6.922315932679375,\n",
       " 6.418315662487569,\n",
       " 6.551518867143379,\n",
       " 7.081919345763022,\n",
       " 6.313034946012681,\n",
       " 6.355203684712051,\n",
       " 6.480038787148177,\n",
       " 6.434240863579525,\n",
       " 6.56856134257765,\n",
       " 6.185227533378252,\n",
       " 6.3080764672581715,\n",
       " 6.0927820273600775,\n",
       " 5.547195161939986,\n",
       " 5.69689746241874,\n",
       " 6.161330333591955,\n",
       " 5.920117373898428,\n",
       " 5.839062735748358,\n",
       " 4.941220204107849,\n",
       " 5.058563282732229,\n",
       " 6.182191842387167,\n",
       " 5.323691961282026,\n",
       " 5.124943186902189,\n",
       " 5.159322737594474,\n",
       " 5.1024892012990355,\n",
       " 5.2687919408268735,\n",
       " 5.416981797122439,\n",
       " 5.48061968660275,\n",
       " 5.516283895914345,\n",
       " 4.960393126739465,\n",
       " 4.706761523023032,\n",
       " 5.293064757642681,\n",
       " 4.865774054527363,\n",
       " 4.563948032000887,\n",
       " 4.370119336077953,\n",
       " 4.1786951220797315,\n",
       " 4.561519164546552,\n",
       " 4.620203783270639,\n",
       " 4.803744765345593,\n",
       " 4.73526031859821,\n",
       " 4.385438086245462,\n",
       " 4.597715886830328,\n",
       " 4.084278942592076,\n",
       " 4.345339050968498,\n",
       " 5.146847601379132,\n",
       " 4.56417805198776,\n",
       " 4.687514830879273,\n",
       " 4.4473195538415125,\n",
       " 3.901397084317986,\n",
       " 4.5298287952146215,\n",
       " 4.410046534581106,\n",
       " 4.186133920737276,\n",
       " 4.2326970206909476,\n",
       " 4.369317644852158,\n",
       " 3.7387501010665325,\n",
       " 4.114498460049386,\n",
       " 4.140087405380873,\n",
       " 4.042870289133563,\n",
       " 4.590187508336226,\n",
       " 3.9016206880424393,\n",
       " 3.764808497067205,\n",
       " 3.7150089303790295,\n",
       " 4.023262614062733,\n",
       " 3.655432405352911,\n",
       " 3.8640952720469754,\n",
       " 3.606338535415281,\n",
       " 3.737429449031849,\n",
       " 3.413655499038762,\n",
       " 3.680126955408344,\n",
       " 4.433766815627312,\n",
       " 4.2899597243245084,\n",
       " 3.92081576339967,\n",
       " 4.414583643105985,\n",
       " 3.3032935085735113,\n",
       " 4.017873279177855,\n",
       " 3.6212978766838346,\n",
       " 3.717592258133316,\n",
       " 3.4648444492057213,\n",
       " 3.3119982536648056,\n",
       " 3.0609733173627007,\n",
       " 4.182851680418433,\n",
       " 3.413636880443178,\n",
       " 3.837913578508485,\n",
       " 3.915096824975862,\n",
       " 3.448124520914068,\n",
       " 3.198396953876285,\n",
       " 3.172317665755614,\n",
       " 3.1735228166779166,\n",
       " 3.377244308388017,\n",
       " 3.378877478790118,\n",
       " 3.858294668844102,\n",
       " 3.4540314986720313,\n",
       " 3.448384621421431,\n",
       " 3.1672276226392158,\n",
       " 3.7644441493092944,\n",
       " 3.335228469297496,\n",
       " 3.9407961753682867,\n",
       " 2.410661697493023,\n",
       " 2.7742547148184813,\n",
       " 3.689371006878747,\n",
       " 3.4219973824634327,\n",
       " 3.449534185530166,\n",
       " 2.837757145876824,\n",
       " 3.4143918770909876,\n",
       " 3.101435860770701,\n",
       " 3.5646521392290493,\n",
       " 3.192373884207505,\n",
       " 3.0175533600442783,\n",
       " 3.384478647208171,\n",
       " 3.020749242412596,\n",
       " 2.9881115231993896,\n",
       " 3.3515637572028596,\n",
       " 3.042475600837762,\n",
       " 3.3530500623787187,\n",
       " 3.158515261221088,\n",
       " 2.972158706732754,\n",
       " 3.1441983292855578,\n",
       " 2.9274202890537917,\n",
       " 3.0524528362837327,\n",
       " 3.067888932517389,\n",
       " 2.782434540061025,\n",
       " 2.8557683484402725,\n",
       " 2.718576881120778,\n",
       " 2.557863966012757,\n",
       " 3.1489524775532045,\n",
       " 2.7694735074375445,\n",
       " 3.284523357924723,\n",
       " 2.9159578269994615,\n",
       " 3.3223919371434802,\n",
       " 3.1995510038790744,\n",
       " 2.9102926222868035,\n",
       " 2.229136300878303,\n",
       " 2.6230161146980064,\n",
       " 3.6088428739595155,\n",
       " 3.6932409458330544,\n",
       " 2.617388627575873,\n",
       " 2.2890799265317323,\n",
       " 3.1095606231198105,\n",
       " 2.3896208638329823,\n",
       " 3.201562063350889,\n",
       " 2.9811770664300807,\n",
       " 2.690592939927308,\n",
       " 2.734888777895481,\n",
       " 2.488715767610591,\n",
       " 2.6185557231612204,\n",
       " 2.9352685143940915,\n",
       " 2.0151404122729257]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0944, 0.4202, 0.6146]\n",
      "[0.0974, 0.4011, 0.6053]\n"
     ]
    }
   ],
   "source": [
    "# 7) ì¶”ê°€ë¡œ, í•˜ë‚˜ì˜ ì—í­ë§ˆë‹¤ í•™ìŠµ ì •í™•ë„ì™€ í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.(êµì¬ p.144 ì°¸ê³ )\n",
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. 3ì¸µ ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°\n",
    "ì´ë²ˆì—” 3ì¸µ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê³¼ì •ì„ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. Q3ê³¼ ë™ì¼í•œ ê³¼ì •ì„ ê±°ì¹˜ë©´ ë©ë‹ˆë‹¤. ë‹¨, ì´ë²ˆ ì‹ ê²½ë§ì˜ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.(input layerì˜ ë…¸ë“œëŠ” 784ì´ê³ ,ì€ë‹‰ì¸µì€ ê°ê° 100, 50ì´ë©° ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ ë…¸ë“œëŠ” 10ì…ë‹ˆë‹¤.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ì´ì œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ 3ì¸µ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ì—¬ MNIST ë°ì´í„°ì— ëŒ€í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ThreeLayerNetì´ë¼ëŠ” í´ë˜ìŠ¤ ì•ˆì—ì„œ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤. Ipynb íŒŒì¼ ì•ˆì— ThreeLayerNetì´ë¼ëŠ” ì´ë¦„ì˜ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì£¼ì„¸ìš”\n",
    "class ThreeLayerNet():\n",
    "# 2) 1ë²ˆì—ì„œ ë§Œë“  TwoLayerNet í´ë˜ìŠ¤ ì•ˆì— ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê° í•¨ìˆ˜ì˜ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ì•Œë§ê²Œ ì½”ë“œë¥¼ êµ¬í˜„í•´ ì£¼ì„¸ìš”.\n",
    "    # a. í•´ë‹¹ í•¨ìˆ˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. Input_sizeëŠ” ì…ë ¥ì¸µì˜ ë…¸ë“œ, hidden_sizeëŠ” ì€ë‹‰ì¸µì˜ ë…¸ë“œ, output_sizeëŠ” ì¶œë ¥ì¸µì˜ ë…¸ë“œ ìˆ˜ ì…ë‹ˆë‹¤. ì€ë‹‰ì¸µì˜ ë…¸ë“œ ìˆ˜ëŠ” listì˜ í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤. (ex.[100, 50]) ì´ë¥¼ ì°¸ê³ í•œ í›„ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”. (í•´ë‹¹ í•¨ìˆ˜ëŠ” í´ë˜ìŠ¤ì—ì„œ ì´ˆê¸°í™”ê°€ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— ë¦¬í„´ê°’ì´ ì—†ìŠµë‹ˆë‹¤.)\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size[0],hidden_size[1])\n",
    "        self.params['b2'] = np.zeros(hidden_size[1])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size[1],output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    # b. Q3ì²˜ëŸ¼ predict, accuracy, loss, numerical_gradient í•¨ìˆ˜ë¥¼ ì±„ì›Œì£¼ì„¸ìš”.\n",
    "    def predict(self, x):\n",
    "        W1,W2,W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1,b2,b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2,W3)+b3\n",
    "        y = softmax(a3)\n",
    "        return y\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 50) (50,) (50, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# 3) ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ThreeLayerNetì„ ì‹¤í–‰ì‹œí‚¤ë ¤ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ê°€ì¤‘ì¹˜ì˜ shapeê°€ ì˜¬ë°”ë¥´ê²Œ ì¶œë ¥ë˜ëŠ” ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "net = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape, net.params['W3'].shape, net.params['b3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.1, 0.0958\n",
      "train acc, test acc : 0.099375, 0.1009\n"
     ]
    }
   ],
   "source": [
    "# 4) ë³¸ê²©ì ìœ¼ë¡œ MNIST ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. EpochsëŠ” 1ë²ˆë§Œ ëŒê³ , í•œ epochë‹¹ ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ì˜ í¬ê¸°(batch size)ëŠ” 128ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•˜ì„¸ìš”. í•´ë‹¹ ì½”ë“œ ì•„ë˜ìª½ì— ì´ì–´ì„œ êµ¬í˜„í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "import copy\n",
    "\n",
    "#ì¼ë¶€ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "subset_size = 12800\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "epoch_num = 1  # ì›í•˜ëŠ” epoch ìˆ˜\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #ë¹„êµë¥¼ ìœ„í•œ ì´ˆê¸°ê°’ ì €ì¥\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.899649556691728\n",
      "4.8433396174813765\n",
      "-1.943690060789649\n"
     ]
    }
   ],
   "source": [
    "# W1 ì´ˆê¸°ê°’ (ë¹„êµë¥¼ ìœ„í•´ sumì—°ì‚°ëœ ê°’ìœ¼ë¡œ ì¶œë ¥)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 í˜„ì¬ê°’\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# ì°¨ì´ ë¹„êµ\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.22846154506918,\n",
       " 9.409232733207835,\n",
       " 9.847872665308344,\n",
       " 8.978458827239526,\n",
       " 8.997127235349055,\n",
       " 8.801956561860145,\n",
       " 9.095912950727836,\n",
       " 9.171282628885649,\n",
       " 9.52650694477084,\n",
       " 9.734651494213777,\n",
       " 8.937961408308698,\n",
       " 9.382178287653707,\n",
       " 9.395780958308205,\n",
       " 9.031771217557571,\n",
       " 9.102811321227929,\n",
       " 8.0044743879103,\n",
       " 9.13097660340922,\n",
       " 9.64298405058637,\n",
       " 9.237727476739007,\n",
       " 8.04459836522955,\n",
       " 9.097296447624327,\n",
       " 9.449206220933014,\n",
       " 8.985048929437028,\n",
       " 10.052724146642756,\n",
       " 9.22948947336451,\n",
       " 9.346240115553359,\n",
       " 9.000379750914746,\n",
       " 8.718195814509336,\n",
       " 9.090859973674046,\n",
       " 8.14259067455781,\n",
       " 9.247117402059562,\n",
       " 8.691616095679962,\n",
       " 8.848032347069427,\n",
       " 8.226556199125277,\n",
       " 9.21362526100964,\n",
       " 8.744284569970826,\n",
       " 9.058842931015644,\n",
       " 8.3283691415599,\n",
       " 8.93792631471601,\n",
       " 8.31640314223811,\n",
       " 9.603657928657151,\n",
       " 9.27610002391576,\n",
       " 9.084423651075804,\n",
       " 8.044630383502847,\n",
       " 8.819819970824728,\n",
       " 8.534620527974822,\n",
       " 8.72364827926604,\n",
       " 9.479959925130075,\n",
       " 9.053444732350824,\n",
       " 8.995175697074982,\n",
       " 9.290865552906531,\n",
       " 9.095402464155796,\n",
       " 8.774921517143211,\n",
       " 8.96761705091494,\n",
       " 9.011539380700434,\n",
       " 9.064230616242142,\n",
       " 8.926937483270159,\n",
       " 9.266447889058911,\n",
       " 8.444915987706635,\n",
       " 8.171713228116772,\n",
       " 8.801743188557957,\n",
       " 9.135312465828365,\n",
       " 9.49498977369831,\n",
       " 9.287233514873774,\n",
       " 9.087098166002031,\n",
       " 8.494281701320466,\n",
       " 8.777232378615059,\n",
       " 9.501463038975853,\n",
       " 8.961097180087833,\n",
       " 9.352074007243449,\n",
       " 8.962467063272207,\n",
       " 9.063854335518919,\n",
       " 8.762023927267002,\n",
       " 8.643895514708943,\n",
       " 9.375122928025032,\n",
       " 8.869488057849733,\n",
       " 9.123886740105512,\n",
       " 8.61030750852657,\n",
       " 9.129274610779849,\n",
       " 8.573653387565844,\n",
       " 8.938829529073736,\n",
       " 9.103584683381616,\n",
       " 9.44201418273116,\n",
       " 9.611471075385165,\n",
       " 9.203471041826939,\n",
       " 9.425292397135335,\n",
       " 8.743735647694468,\n",
       " 8.865865339134427,\n",
       " 8.790066021169684,\n",
       " 9.12482321822548,\n",
       " 9.252240427635371,\n",
       " 9.273991683240414,\n",
       " 8.921610238424467,\n",
       " 9.22275544158671,\n",
       " 8.156303508646506,\n",
       " 9.579365787939839,\n",
       " 9.523100077225264,\n",
       " 9.142608209081827,\n",
       " 9.146410280178177,\n",
       " 8.268784517854145]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.099375]\n",
      "[0.0958, 0.1009]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3ì¸µì‹ ê²½ë§) í™œì„±í™” í•¨ìˆ˜ë§Œ sigmoidì—ì„œ reluë¡œ ë°”ê¾¸ê³  ì¬ì‹¤í–‰!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.099375, 0.1009\n",
      "train acc, test acc : 0.3690625, 0.3634\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerNet():\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size[0],hidden_size[1])\n",
    "        self.params['b2'] = np.zeros(hidden_size[1])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size[1],output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    # b. Q3ì²˜ëŸ¼ predict, accuracy, loss, numerical_gradient í•¨ìˆ˜ë¥¼ ì±„ì›Œì£¼ì„¸ìš”.\n",
    "    def predict(self, x):\n",
    "        W1,W2,W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1,b2,b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        z2 = relu(a2)\n",
    "        a3 = np.dot(z2,W3)+b3\n",
    "        y = softmax(a3)\n",
    "        return y\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "import copy\n",
    "\n",
    "#ì¼ë¶€ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "subset_size = 12800\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "epoch_num = 1  # ì›í•˜ëŠ” epoch ìˆ˜\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #ë¹„êµë¥¼ ìœ„í•œ ì´ˆê¸°ê°’ ì €ì¥\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9509526464380271\n",
      "-28.780734249493413\n",
      "26.82978160305538\n"
     ]
    }
   ],
   "source": [
    "# W1 ì´ˆê¸°ê°’ (ë¹„êµë¥¼ ìœ„í•´ sumì—°ì‚°ëœ ê°’ìœ¼ë¡œ ì¶œë ¥)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 í˜„ì¬ê°’\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# ì°¨ì´ ë¹„êµ\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.397513826800129,\n",
       " 11.31209118987985,\n",
       " 9.933224544520629,\n",
       " 9.287576777032472,\n",
       " 9.893111019232025,\n",
       " 8.842578211236898,\n",
       " 10.263969898714013,\n",
       " 9.679057973628723,\n",
       " 9.724913153112322,\n",
       " 9.053697796163702,\n",
       " 10.07288389720543,\n",
       " 9.721100220761677,\n",
       " 9.61998597695606,\n",
       " 9.149386527674409,\n",
       " 9.098549353965385,\n",
       " 9.852076822098592,\n",
       " 9.248633961284355,\n",
       " 9.187945329580826,\n",
       " 9.074728162624783,\n",
       " 9.628564686103992,\n",
       " 10.118749197917882,\n",
       " 9.149461089323037,\n",
       " 10.000946806610678,\n",
       " 9.01518834076326,\n",
       " 10.133714707199642,\n",
       " 8.981618896214295,\n",
       " 9.443771134050797,\n",
       " 9.160638610309165,\n",
       " 9.486961193397693,\n",
       " 9.606360774425275,\n",
       " 9.395923528506437,\n",
       " 9.465657072767298,\n",
       " 9.036176668261703,\n",
       " 9.716513639817425,\n",
       " 9.569464892056445,\n",
       " 8.977453311460472,\n",
       " 8.752065279162187,\n",
       " 9.118176660016612,\n",
       " 8.817604205635774,\n",
       " 9.336090747622801,\n",
       " 8.269176183613673,\n",
       " 7.868050534902874,\n",
       " 9.197797757070745,\n",
       " 9.359002361181794,\n",
       " 8.923744571311353,\n",
       " 9.26156107512939,\n",
       " 9.134836027178835,\n",
       " 8.943283866597366,\n",
       " 9.184607053355712,\n",
       " 9.267559903420771,\n",
       " 9.770529461380605,\n",
       " 8.716753598461837,\n",
       " 8.78436897191144,\n",
       " 9.362895953557624,\n",
       " 9.431606380513818,\n",
       " 8.301334756990308,\n",
       " 9.70627349101921,\n",
       " 8.33449941431901,\n",
       " 8.736570490976401,\n",
       " 9.322702337180463,\n",
       " 9.157269454437039,\n",
       " 8.740834830861168,\n",
       " 8.109370153010047,\n",
       " 7.784626112816676,\n",
       " 8.223457602895838,\n",
       " 7.893254836098331,\n",
       " 8.027959574509769,\n",
       " 7.5770258892754905,\n",
       " 7.349514662321612,\n",
       " 7.994452880032538,\n",
       " 8.769977652467462,\n",
       " 9.31409846187184,\n",
       " 8.261656114947193,\n",
       " 7.878859542123932,\n",
       " 7.166578782031085,\n",
       " 7.389497075856799,\n",
       " 6.9368491131135475,\n",
       " 6.366393522378493,\n",
       " 5.796010326450833,\n",
       " 6.062323451928108,\n",
       " 6.435839217733234,\n",
       " 6.143496155463577,\n",
       " 6.253049461729542,\n",
       " 6.600706934783104,\n",
       " 11.015114593189406,\n",
       " 8.271296250651483,\n",
       " 7.994873353597523,\n",
       " 7.5442217981304385,\n",
       " 6.876994067144965,\n",
       " 6.321251495220091,\n",
       " 6.398027917861169,\n",
       " 6.32167698104243,\n",
       " 5.550671995868786,\n",
       " 4.963372961010611,\n",
       " 5.817805220329948,\n",
       " 7.981068712501841,\n",
       " 8.24994687650658,\n",
       " 7.901900155728547,\n",
       " 6.85831642111998,\n",
       " 5.909636956016945]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.099375, 0.3690625]\n",
      "[0.1009, 0.3634]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3ì¸µì‹ ê²½ë§ì˜ ê²½ìš° ë°ì´í„°ê°€ ì ì–´ì„œì¸ì§€(ì‹œê°„ìƒ ì¼ë¶€ë§Œì‚¬ìš©), í•™ìŠµíšŸìˆ˜ê°€ ë” ë§ì´ í•„ìš”í•´ì„œì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ, 2ì¸µì‹ ê²½ë§ê³¼ ë‹¤ë¥´ê²Œ 1epochë§Œì— ìœ ì˜í•œ ì„±ëŠ¥ê°œì„ ì´ ì´ë£¨ì–´ì§€ì§€ì•ŠìŒ.\n",
    "#### ë‹¤ë§Œ, í™œì„±í™”í•¨ìˆ˜ë¥¼ sigmoidì—ì„œ reluë¡œ ë³€ê²½ í›„ ì‹¤í–‰í•´ë³´ë‹ˆ ì„±ëŠ¥í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŒ. (ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ë°œìƒí•˜ëŠ” ê¸°ìš¸ê¸°ì†Œì‹¤ë¬¸ì œê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
