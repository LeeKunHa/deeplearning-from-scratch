{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 손실함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 이제 본격적으로 손실함수를 구현하려고 합니다. 먼저, 오차제곱합을 구현하겠습니다. 수식은 아래와 같습니다. 아래 수식을 참고하여 오차 제곱합을 계산하는 것을 구현하세요. 함수 원형은 아래와 같습니다.(y, t는 각각 예측값과 실제값)\n",
    "def sum_squares_error(y, t): #정답레이블뿐만 아니라, 오답레이블의 수치도 반영(원핫인코딩)\n",
    "    return (0.5) * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 추가로, 교차 엔트로피(cross entropy)를 구현하려고 합니다. 아래 수식을 참고하여 교차 엔트로피를 계산하는 것을 구현하세요. 함수 원형은 아래와 같습니다. (단, tk와 yk는 각각 k번째 데이터의 실제 값과 예측 값)\n",
    "#정답레이블이 원핫인코딩인 경우\n",
    "def cross_entropy_error_one_hot(y, t):\n",
    "    if y.dim==1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta)) / batch_size #정답라벨링 이외의 값은 버려짐(*0)\n",
    "\n",
    "\n",
    "#정답레이블이 단일값인 경우\n",
    "def cross_entropy_error(y, t): #이 구현의 경우 원핫인코딩에서 t가 0인 원소는 교차엔트로피 오차도 0이므로 그 계산을 무시한다는 것이 핵심. 즉 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산!\n",
    "    if y.dim==1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size), t] + delta)) / batch_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. 수치 미분 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 이제 편미분을 구현하려고 합니다. 목적함수 f와 미지수 x가 주어졌을 때, 편미분을 하는 과정을 구현하세요. 함수 원형은 아래와 같습니다. (단, 구현하실 때 가중치가 1차원의 형태가 아닌 2차원의 형태가 들어왔다고 가정하고 구현하셔야 합니다.) hint : np.nditer 활용\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 매개변수를 갱신하기 위해 경사 하강법(gradient descent)를 이용하려고 합니다. 경사하강법을 적용할 때 갱신되는 매개변수의 식은 아래와 같습니다. 𝒙𝒌+𝟏 = 𝒙𝒌 − 𝜶 × 𝝏𝒇/𝝏𝒙𝒌 위 식을 이용하여 경사 하강법을 구현하세요. 함수 원형은 아래와 같습니다. (f : 목적함수, init_x : 매개변수의 초기값, lr : alpha(학습률), step_num : 경사하강법을 진행할 횟수)\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x) #\"이만큼 이동하세요\"\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 2층 신경망을 통해 가중치 업데이트하기\n",
    "MNIST 데이터를 이용하여 아래와 같은 신경망을 구현해 보려고 합니다.(순서대로 아래 쪽 숫자는 784, 100, 10입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 아래와 같은 코드를 불러오세요.\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 이제 위 그림과 같은 2층 신경망을 구현하여 MNIST 데이터에 대하여 가중치를 업데이트 해보려고 합니다. 이 과정은 TwoLayerNet이라는 클래스 안에서 진행될 예정입니다. Ipynb 파일 안에 TwoLayerNet이라는 이름의 클래스를 하나 만들어 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 2번에서 만든 TwoLayerNet 클래스 안에 아래와 같은 함수를 구현하려고 합니다. 각 함수의 설명을 참고하여 알맞게 코드를 구현해 주세요.(weight_init_std도 활용해주세요)\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # a. 해당 함수는 가중치를 초기화 하는 함수입니다. Input_size는 입력층의 노드, hidden_size는 은닉층의 노드, output_size는 출력층의 노드 수 입니다. 이를 참고한 후 딕셔너리를 이용하여 가중치를 초기화하는 함수를 구현하세요. (해당 함수는 클래스에서 초기화가 일어나는 부분이기 때문에 리턴값이 없습니다.)\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    # b. 해당 함수는 초기화된 가중치를 이용하여 신경망의 연산 과정이 일어나는 부분입니다. 지난 주와 동일한 방법으로 결과를 리턴하도록 구현하세요. 리턴해야 하는 것은 y(예측값) 입니다.\n",
    "    def predict(self, x):\n",
    "        W1,W2 = self.params['W1'], self.params['W2']\n",
    "        b1,b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # c. 해당 함수는 예측 결과와 실제 정답 간의 정확도를 계산하는 부분입니다. X와 t는 각각 입력 데이터와 정답 레이블입니다. 이를 참고하여 정확도 accuracy를 계산하고 리턴하세요.\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # d. 해당 함수는 예측 결과와 실제 정답 간의 손실을 계산하는 부분입니다. 계산 과정을 구현하고 loss를 리턴하세요. Loss function은 자율적으로 선택하시고, 해당 loss function을 선택한 이유를 설명하세요.\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    # e. 해당 함수는 계산한 loss를 바탕으로 가중치를 업데이트하는 함수입니다. 딕셔너리를 이용하여 업데이트된 가중치를 리턴하도록 구현하세요.\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# 4) 이제 3번에서 구현한 클래스를 바탕으로 연산 과정을 해보려고 합니다. 아래 코드를 실행하여서 가중치의 shape가 올바르게 출력되는 지 확인하세요.\n",
    "#from common.two_layer_net import * # 위 함수와 똑같지만 충돌발생하기때문에 주석처리\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.0944, 0.0974\n",
      "train acc, test acc : 0.4202, 0.4011\n",
      "train acc, test acc : 0.6146, 0.6053\n"
     ]
    }
   ],
   "source": [
    "# 6) 본격적으로 MNIST 데이터를 이용하여 가중치를 업데이트 해보려고 합니다.(train data 이용) Epochs는 2번만 돌고, 한 epoch당 처리하는 배치의 크기(batch size)는 100으로 지정하여 가중치가 업데이트 되는 과정을 구현합니다. 또한, 가중치 w1이 초기값과 얼마나 차이가 나는 지를 출력하는 코드를 작성하세요.\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('')\n",
    "from dataset.mnist import load_mnist\n",
    "#from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "import copy\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "#일부만 가져오기\n",
    "subset_size = 10000\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "epoch_num = 2  # 원하는 epoch 수\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #비교를 위한 초기값 저장\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.8557096720821074\n",
      "-61.76200638274949\n",
      "57.90629671066738\n"
     ]
    }
   ],
   "source": [
    "# W1 초기값 (비교를 위해 sum연산된 값으로 출력)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 현재값\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# 차이 비교\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.789917477035026,\n",
       " 9.402878615095732,\n",
       " 8.53129252003895,\n",
       " 8.956632884256209,\n",
       " 9.155539960960349,\n",
       " 9.17339682078703,\n",
       " 9.393068895443568,\n",
       " 9.32320104269466,\n",
       " 8.871058476502549,\n",
       " 8.748610338629673,\n",
       " 9.860914834073586,\n",
       " 8.708943996540238,\n",
       " 8.528062875459552,\n",
       " 9.190757737328298,\n",
       " 8.668473667912115,\n",
       " 8.74836678599071,\n",
       " 9.628166929402846,\n",
       " 9.014904746447389,\n",
       " 8.920513072717156,\n",
       " 8.636347240924136,\n",
       " 8.494875067946344,\n",
       " 8.504336803104428,\n",
       " 9.415620445784853,\n",
       " 8.532462989336825,\n",
       " 8.251651851354955,\n",
       " 8.334178857515965,\n",
       " 7.977338374885249,\n",
       " 8.508595628327468,\n",
       " 9.037225948140746,\n",
       " 8.52239796478214,\n",
       " 9.222595521753204,\n",
       " 9.103690415217098,\n",
       " 8.989468279386891,\n",
       " 7.676927544145608,\n",
       " 8.539393538062177,\n",
       " 8.981825598728424,\n",
       " 7.954785772585041,\n",
       " 7.660103009255141,\n",
       " 8.763949356144254,\n",
       " 8.86246195821568,\n",
       " 7.975586171879179,\n",
       " 7.947219773391989,\n",
       " 7.339098352617026,\n",
       " 9.056825281589106,\n",
       " 7.326638564813185,\n",
       " 7.176040996159823,\n",
       " 8.178553661202754,\n",
       " 7.7584252656771175,\n",
       " 7.359936547510157,\n",
       " 7.742716069808982,\n",
       " 7.458780545428644,\n",
       " 7.512987486508391,\n",
       " 7.268141924098149,\n",
       " 7.11724159923928,\n",
       " 6.922315932679375,\n",
       " 6.418315662487569,\n",
       " 6.551518867143379,\n",
       " 7.081919345763022,\n",
       " 6.313034946012681,\n",
       " 6.355203684712051,\n",
       " 6.480038787148177,\n",
       " 6.434240863579525,\n",
       " 6.56856134257765,\n",
       " 6.185227533378252,\n",
       " 6.3080764672581715,\n",
       " 6.0927820273600775,\n",
       " 5.547195161939986,\n",
       " 5.69689746241874,\n",
       " 6.161330333591955,\n",
       " 5.920117373898428,\n",
       " 5.839062735748358,\n",
       " 4.941220204107849,\n",
       " 5.058563282732229,\n",
       " 6.182191842387167,\n",
       " 5.323691961282026,\n",
       " 5.124943186902189,\n",
       " 5.159322737594474,\n",
       " 5.1024892012990355,\n",
       " 5.2687919408268735,\n",
       " 5.416981797122439,\n",
       " 5.48061968660275,\n",
       " 5.516283895914345,\n",
       " 4.960393126739465,\n",
       " 4.706761523023032,\n",
       " 5.293064757642681,\n",
       " 4.865774054527363,\n",
       " 4.563948032000887,\n",
       " 4.370119336077953,\n",
       " 4.1786951220797315,\n",
       " 4.561519164546552,\n",
       " 4.620203783270639,\n",
       " 4.803744765345593,\n",
       " 4.73526031859821,\n",
       " 4.385438086245462,\n",
       " 4.597715886830328,\n",
       " 4.084278942592076,\n",
       " 4.345339050968498,\n",
       " 5.146847601379132,\n",
       " 4.56417805198776,\n",
       " 4.687514830879273,\n",
       " 4.4473195538415125,\n",
       " 3.901397084317986,\n",
       " 4.5298287952146215,\n",
       " 4.410046534581106,\n",
       " 4.186133920737276,\n",
       " 4.2326970206909476,\n",
       " 4.369317644852158,\n",
       " 3.7387501010665325,\n",
       " 4.114498460049386,\n",
       " 4.140087405380873,\n",
       " 4.042870289133563,\n",
       " 4.590187508336226,\n",
       " 3.9016206880424393,\n",
       " 3.764808497067205,\n",
       " 3.7150089303790295,\n",
       " 4.023262614062733,\n",
       " 3.655432405352911,\n",
       " 3.8640952720469754,\n",
       " 3.606338535415281,\n",
       " 3.737429449031849,\n",
       " 3.413655499038762,\n",
       " 3.680126955408344,\n",
       " 4.433766815627312,\n",
       " 4.2899597243245084,\n",
       " 3.92081576339967,\n",
       " 4.414583643105985,\n",
       " 3.3032935085735113,\n",
       " 4.017873279177855,\n",
       " 3.6212978766838346,\n",
       " 3.717592258133316,\n",
       " 3.4648444492057213,\n",
       " 3.3119982536648056,\n",
       " 3.0609733173627007,\n",
       " 4.182851680418433,\n",
       " 3.413636880443178,\n",
       " 3.837913578508485,\n",
       " 3.915096824975862,\n",
       " 3.448124520914068,\n",
       " 3.198396953876285,\n",
       " 3.172317665755614,\n",
       " 3.1735228166779166,\n",
       " 3.377244308388017,\n",
       " 3.378877478790118,\n",
       " 3.858294668844102,\n",
       " 3.4540314986720313,\n",
       " 3.448384621421431,\n",
       " 3.1672276226392158,\n",
       " 3.7644441493092944,\n",
       " 3.335228469297496,\n",
       " 3.9407961753682867,\n",
       " 2.410661697493023,\n",
       " 2.7742547148184813,\n",
       " 3.689371006878747,\n",
       " 3.4219973824634327,\n",
       " 3.449534185530166,\n",
       " 2.837757145876824,\n",
       " 3.4143918770909876,\n",
       " 3.101435860770701,\n",
       " 3.5646521392290493,\n",
       " 3.192373884207505,\n",
       " 3.0175533600442783,\n",
       " 3.384478647208171,\n",
       " 3.020749242412596,\n",
       " 2.9881115231993896,\n",
       " 3.3515637572028596,\n",
       " 3.042475600837762,\n",
       " 3.3530500623787187,\n",
       " 3.158515261221088,\n",
       " 2.972158706732754,\n",
       " 3.1441983292855578,\n",
       " 2.9274202890537917,\n",
       " 3.0524528362837327,\n",
       " 3.067888932517389,\n",
       " 2.782434540061025,\n",
       " 2.8557683484402725,\n",
       " 2.718576881120778,\n",
       " 2.557863966012757,\n",
       " 3.1489524775532045,\n",
       " 2.7694735074375445,\n",
       " 3.284523357924723,\n",
       " 2.9159578269994615,\n",
       " 3.3223919371434802,\n",
       " 3.1995510038790744,\n",
       " 2.9102926222868035,\n",
       " 2.229136300878303,\n",
       " 2.6230161146980064,\n",
       " 3.6088428739595155,\n",
       " 3.6932409458330544,\n",
       " 2.617388627575873,\n",
       " 2.2890799265317323,\n",
       " 3.1095606231198105,\n",
       " 2.3896208638329823,\n",
       " 3.201562063350889,\n",
       " 2.9811770664300807,\n",
       " 2.690592939927308,\n",
       " 2.734888777895481,\n",
       " 2.488715767610591,\n",
       " 2.6185557231612204,\n",
       " 2.9352685143940915,\n",
       " 2.0151404122729257]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0944, 0.4202, 0.6146]\n",
      "[0.0974, 0.4011, 0.6053]\n"
     ]
    }
   ],
   "source": [
    "# 7) 추가로, 하나의 에폭마다 학습 정확도와 테스트 정확도를 저장할 수 있도록 코드를 추가하세요.(교재 p.144 참고)\n",
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. 3층 신경망 구현하기\n",
    "이번엔 3층 신경망의 가중치를 업데이트 하는 과정을 해보려고 합니다. Q3과 동일한 과정을 거치면 됩니다. 단, 이번 신경망의 구조는 아래와 같습니다.(input layer의 노드는 784이고,은닉층은 각각 100, 50이며 마지막 출력층 노드는 10입니다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 이제 위 그림과 같은 3층 신경망을 구현하여 MNIST 데이터에 대하여 가중치를 업데이트 해보려고 합니다. 이 과정은 ThreeLayerNet이라는 클래스 안에서 진행될 예정입니다. Ipynb 파일 안에 ThreeLayerNet이라는 이름의 클래스를 하나 만들어 주세요\n",
    "class ThreeLayerNet():\n",
    "# 2) 1번에서 만든 TwoLayerNet 클래스 안에 아래와 같은 함수를 구현하려고 합니다. 각 함수의 설명을 참고하여 알맞게 코드를 구현해 주세요.\n",
    "    # a. 해당 함수는 가중치를 초기화 하는 함수입니다. Input_size는 입력층의 노드, hidden_size는 은닉층의 노드, output_size는 출력층의 노드 수 입니다. 은닉층의 노드 수는 list의 형태로 저장됩니다. (ex.[100, 50]) 이를 참고한 후 딕셔너리를 이용하여 가중치를 초기화 하는 함수를 구현하세요. (해당 함수는 클래스에서 초기화가 일어나는 부분이기 때문에 리턴값이 없습니다.)\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size[0],hidden_size[1])\n",
    "        self.params['b2'] = np.zeros(hidden_size[1])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size[1],output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    # b. Q3처럼 predict, accuracy, loss, numerical_gradient 함수를 채워주세요.\n",
    "    def predict(self, x):\n",
    "        W1,W2,W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1,b2,b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2,W3)+b3\n",
    "        y = softmax(a3)\n",
    "        return y\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 50) (50,) (50, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# 3) 지금까지 구현한 ThreeLayerNet을 실행시키려고 합니다. 아래 코드를 실행하여 가중치의 shape가 올바르게 출력되는 지 확인하세요.\n",
    "net = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape, net.params['W3'].shape, net.params['b3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.1, 0.0958\n",
      "train acc, test acc : 0.099375, 0.1009\n"
     ]
    }
   ],
   "source": [
    "# 4) 본격적으로 MNIST 데이터를 이용하여 가중치를 업데이트 해보려고 합니다. Epochs는 1번만 돌고, 한 epoch당 처리하는 배치의 크기(batch size)는 128으로 지정하여 가중치가 업데이트 되는 과정을 구현하세요. 해당 코드 아래쪽에 이어서 구현하시면 됩니다.\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "import copy\n",
    "\n",
    "#일부만 가져오기\n",
    "subset_size = 12800\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "epoch_num = 1  # 원하는 epoch 수\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #비교를 위한 초기값 저장\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.899649556691728\n",
      "4.8433396174813765\n",
      "-1.943690060789649\n"
     ]
    }
   ],
   "source": [
    "# W1 초기값 (비교를 위해 sum연산된 값으로 출력)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 현재값\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# 차이 비교\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.22846154506918,\n",
       " 9.409232733207835,\n",
       " 9.847872665308344,\n",
       " 8.978458827239526,\n",
       " 8.997127235349055,\n",
       " 8.801956561860145,\n",
       " 9.095912950727836,\n",
       " 9.171282628885649,\n",
       " 9.52650694477084,\n",
       " 9.734651494213777,\n",
       " 8.937961408308698,\n",
       " 9.382178287653707,\n",
       " 9.395780958308205,\n",
       " 9.031771217557571,\n",
       " 9.102811321227929,\n",
       " 8.0044743879103,\n",
       " 9.13097660340922,\n",
       " 9.64298405058637,\n",
       " 9.237727476739007,\n",
       " 8.04459836522955,\n",
       " 9.097296447624327,\n",
       " 9.449206220933014,\n",
       " 8.985048929437028,\n",
       " 10.052724146642756,\n",
       " 9.22948947336451,\n",
       " 9.346240115553359,\n",
       " 9.000379750914746,\n",
       " 8.718195814509336,\n",
       " 9.090859973674046,\n",
       " 8.14259067455781,\n",
       " 9.247117402059562,\n",
       " 8.691616095679962,\n",
       " 8.848032347069427,\n",
       " 8.226556199125277,\n",
       " 9.21362526100964,\n",
       " 8.744284569970826,\n",
       " 9.058842931015644,\n",
       " 8.3283691415599,\n",
       " 8.93792631471601,\n",
       " 8.31640314223811,\n",
       " 9.603657928657151,\n",
       " 9.27610002391576,\n",
       " 9.084423651075804,\n",
       " 8.044630383502847,\n",
       " 8.819819970824728,\n",
       " 8.534620527974822,\n",
       " 8.72364827926604,\n",
       " 9.479959925130075,\n",
       " 9.053444732350824,\n",
       " 8.995175697074982,\n",
       " 9.290865552906531,\n",
       " 9.095402464155796,\n",
       " 8.774921517143211,\n",
       " 8.96761705091494,\n",
       " 9.011539380700434,\n",
       " 9.064230616242142,\n",
       " 8.926937483270159,\n",
       " 9.266447889058911,\n",
       " 8.444915987706635,\n",
       " 8.171713228116772,\n",
       " 8.801743188557957,\n",
       " 9.135312465828365,\n",
       " 9.49498977369831,\n",
       " 9.287233514873774,\n",
       " 9.087098166002031,\n",
       " 8.494281701320466,\n",
       " 8.777232378615059,\n",
       " 9.501463038975853,\n",
       " 8.961097180087833,\n",
       " 9.352074007243449,\n",
       " 8.962467063272207,\n",
       " 9.063854335518919,\n",
       " 8.762023927267002,\n",
       " 8.643895514708943,\n",
       " 9.375122928025032,\n",
       " 8.869488057849733,\n",
       " 9.123886740105512,\n",
       " 8.61030750852657,\n",
       " 9.129274610779849,\n",
       " 8.573653387565844,\n",
       " 8.938829529073736,\n",
       " 9.103584683381616,\n",
       " 9.44201418273116,\n",
       " 9.611471075385165,\n",
       " 9.203471041826939,\n",
       " 9.425292397135335,\n",
       " 8.743735647694468,\n",
       " 8.865865339134427,\n",
       " 8.790066021169684,\n",
       " 9.12482321822548,\n",
       " 9.252240427635371,\n",
       " 9.273991683240414,\n",
       " 8.921610238424467,\n",
       " 9.22275544158671,\n",
       " 8.156303508646506,\n",
       " 9.579365787939839,\n",
       " 9.523100077225264,\n",
       " 9.142608209081827,\n",
       " 9.146410280178177,\n",
       " 8.268784517854145]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.099375]\n",
      "[0.0958, 0.1009]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3층신경망) 활성화 함수만 sigmoid에서 relu로 바꾸고 재실행!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.099375, 0.1009\n",
      "train acc, test acc : 0.3690625, 0.3634\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerNet():\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size[0],hidden_size[1])\n",
    "        self.params['b2'] = np.zeros(hidden_size[1])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size[1],output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    # b. Q3처럼 predict, accuracy, loss, numerical_gradient 함수를 채워주세요.\n",
    "    def predict(self, x):\n",
    "        W1,W2,W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1,b2,b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        z2 = relu(a2)\n",
    "        a3 = np.dot(z2,W3)+b3\n",
    "        y = softmax(a3)\n",
    "        return y\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "import copy\n",
    "\n",
    "#일부만 가져오기\n",
    "subset_size = 12800\n",
    "x_train = x_train[:subset_size]\n",
    "t_train = t_train[:subset_size]\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "epoch_num = 1  # 원하는 epoch 수\n",
    "iter_num = int(epoch_num * train_size / batch_size)\n",
    "network = ThreeLayerNet(input_size = 784, hidden_size = [100, 50], output_size = 10)\n",
    "init_W1 = copy.deepcopy(network.params['W1']) #비교를 위한 초기값 저장\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i == 0 or (i+1) % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9509526464380271\n",
      "-28.780734249493413\n",
      "26.82978160305538\n"
     ]
    }
   ],
   "source": [
    "# W1 초기값 (비교를 위해 sum연산된 값으로 출력)\n",
    "print(np.sum(init_W1))\n",
    "\n",
    "# W1 현재값\n",
    "print(np.sum(network.params['W1']))\n",
    "\n",
    "# 차이 비교\n",
    "print(np.sum(init_W1 - network.params['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.397513826800129,\n",
       " 11.31209118987985,\n",
       " 9.933224544520629,\n",
       " 9.287576777032472,\n",
       " 9.893111019232025,\n",
       " 8.842578211236898,\n",
       " 10.263969898714013,\n",
       " 9.679057973628723,\n",
       " 9.724913153112322,\n",
       " 9.053697796163702,\n",
       " 10.07288389720543,\n",
       " 9.721100220761677,\n",
       " 9.61998597695606,\n",
       " 9.149386527674409,\n",
       " 9.098549353965385,\n",
       " 9.852076822098592,\n",
       " 9.248633961284355,\n",
       " 9.187945329580826,\n",
       " 9.074728162624783,\n",
       " 9.628564686103992,\n",
       " 10.118749197917882,\n",
       " 9.149461089323037,\n",
       " 10.000946806610678,\n",
       " 9.01518834076326,\n",
       " 10.133714707199642,\n",
       " 8.981618896214295,\n",
       " 9.443771134050797,\n",
       " 9.160638610309165,\n",
       " 9.486961193397693,\n",
       " 9.606360774425275,\n",
       " 9.395923528506437,\n",
       " 9.465657072767298,\n",
       " 9.036176668261703,\n",
       " 9.716513639817425,\n",
       " 9.569464892056445,\n",
       " 8.977453311460472,\n",
       " 8.752065279162187,\n",
       " 9.118176660016612,\n",
       " 8.817604205635774,\n",
       " 9.336090747622801,\n",
       " 8.269176183613673,\n",
       " 7.868050534902874,\n",
       " 9.197797757070745,\n",
       " 9.359002361181794,\n",
       " 8.923744571311353,\n",
       " 9.26156107512939,\n",
       " 9.134836027178835,\n",
       " 8.943283866597366,\n",
       " 9.184607053355712,\n",
       " 9.267559903420771,\n",
       " 9.770529461380605,\n",
       " 8.716753598461837,\n",
       " 8.78436897191144,\n",
       " 9.362895953557624,\n",
       " 9.431606380513818,\n",
       " 8.301334756990308,\n",
       " 9.70627349101921,\n",
       " 8.33449941431901,\n",
       " 8.736570490976401,\n",
       " 9.322702337180463,\n",
       " 9.157269454437039,\n",
       " 8.740834830861168,\n",
       " 8.109370153010047,\n",
       " 7.784626112816676,\n",
       " 8.223457602895838,\n",
       " 7.893254836098331,\n",
       " 8.027959574509769,\n",
       " 7.5770258892754905,\n",
       " 7.349514662321612,\n",
       " 7.994452880032538,\n",
       " 8.769977652467462,\n",
       " 9.31409846187184,\n",
       " 8.261656114947193,\n",
       " 7.878859542123932,\n",
       " 7.166578782031085,\n",
       " 7.389497075856799,\n",
       " 6.9368491131135475,\n",
       " 6.366393522378493,\n",
       " 5.796010326450833,\n",
       " 6.062323451928108,\n",
       " 6.435839217733234,\n",
       " 6.143496155463577,\n",
       " 6.253049461729542,\n",
       " 6.600706934783104,\n",
       " 11.015114593189406,\n",
       " 8.271296250651483,\n",
       " 7.994873353597523,\n",
       " 7.5442217981304385,\n",
       " 6.876994067144965,\n",
       " 6.321251495220091,\n",
       " 6.398027917861169,\n",
       " 6.32167698104243,\n",
       " 5.550671995868786,\n",
       " 4.963372961010611,\n",
       " 5.817805220329948,\n",
       " 7.981068712501841,\n",
       " 8.24994687650658,\n",
       " 7.901900155728547,\n",
       " 6.85831642111998,\n",
       " 5.909636956016945]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.099375, 0.3690625]\n",
      "[0.1009, 0.3634]\n"
     ]
    }
   ],
   "source": [
    "print(train_acc_list)\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3층신경망의 경우 데이터가 적어서인지(시간상 일부만사용), 학습횟수가 더 많이 필요해서인지 모르겠지만, 2층신경망과 다르게 1epoch만에 유의한 성능개선이 이루어지지않음.\n",
    "#### 다만, 활성화함수를 sigmoid에서 relu로 변경 후 실행해보니 성능향상을 확인할 수 있었음. (층이 깊어질수록 발생하는 기울기소실문제가 있었을 가능성)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
